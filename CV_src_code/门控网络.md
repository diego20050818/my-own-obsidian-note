---
tags:
  - 基础知识
  - code
  - 门控网络
  - 数学原理
  - LSTM
---

## 概述

门控网络是一种在神经网络中引入门控机制的架构，主要用于控制信息流动。常见的门控网络包括LSTM（长短期记忆网络）和GRU（门控循环单元）。

## 数学原理

### LSTM 门控机制

LSTM包含三个门：输入门、遗忘门和输出门。

#### 遗忘门
控制上一时刻细胞状态保留多少信息：

$$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$$

- $f_t$：遗忘门输出（0-1之间）
- $\sigma$：sigmoid激活函数
- $W_f$：遗忘门权重矩阵
- $h_{t-1}$：上一时刻隐藏状态
- $x_t$：当前时刻输入
- $b_f$：遗忘门偏置项

#### 输入门
控制当前输入信息有多少存入细胞状态：

$$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$$
$$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$$

- $i_t$：输入门输出
- $\tilde{C}_t$：候选细胞状态

#### 细胞状态更新
$$C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t$$

- $C_t$：当前时刻细胞状态
- $C_{t-1}$：上一时刻细胞状态

#### 输出门
控制当前隐藏状态输出：

$$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$$
$$h_t = o_t \cdot \tanh(C_t)$$

### GRU 门控机制

GRU简化了LSTM，只有两个门：重置门和更新门。

#### 更新门
控制历史信息保留程度：

$$z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)$$

#### 重置门
控制历史信息遗忘程度：

$$r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)$$

#### 候选隐藏状态
$$\tilde{h}_t = \tanh(W \cdot [r_t \cdot h_{t-1}, x_t] + b)$$

#### 隐藏状态更新
$$h_t = (1 - z_t) \cdot h_{t-1} + z_t \cdot \tilde{h}_t$$

## 代码实现

### PyTorch LSTM实现

```python
import torch
import torch.nn as nn

class LSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers=1):
        super(LSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # 输入门参数
        self.W_ii = nn.Parameter(torch.Tensor(hidden_size, input_size))
        self.W_hi = nn.Parameter(torch.Tensor(hidden_size, hidden_size))
        self.b_i = nn.Parameter(torch.Tensor(hidden_size))
        
        # 遗忘门参数
        self.W_if = nn.Parameter(torch.Tensor(hidden_size, input_size))
        self.W_hf = nn.Parameter(torch.Tensor(hidden_size, hidden_size))
        self.b_f = nn.Parameter(torch.Tensor(hidden_size))
        
        # 输出门参数
        self.W_io = nn.Parameter(torch.Tensor(hidden_size, input_size))
        self.W_ho = nn.Parameter(torch.Tensor(hidden_size, hidden_size))
        self.b_o = nn.Parameter(torch.Tensor(hidden_size))
        
        # 细胞状态参数
        self.W_ig = nn.Parameter(torch.Tensor(hidden_size, input_size))
        self.W_hg = nn.Parameter(torch.Tensor(hidden_size, hidden_size))
        self.b_g = nn.Parameter(torch.Tensor(hidden_size))
        
        self.init_weights()
    
    def init_weights(self):
        for p in self.parameters():
            if p.data.ndimension() >= 2:
                nn.init.xavier_uniform_(p.data)
            else:
                nn.init.zeros_(p.data)
    
    def forward(self, x, init_states=None):
        batch_size, seq_len, _ = x.size()
        
        if init_states is None:
            h_t = torch.zeros(self.num_layers, batch_size, self.hidden_size)
            c_t = torch.zeros(self.num_layers, batch_size, self.hidden_size)
        else:
            h_t, c_t = init_states
        
        output = []
        for t in range(seq_len):
            x_t = x[:, t, :]
            
            # 输入门
            i_t = torch.sigmoid(x_t @ self.W_ii.T + h_t @ self.W_hi.T + self.b_i)
            
            # 遗忘门
            f_t = torch.sigmoid(x_t @ self.W_if.T + h_t @ self.W_hf.T + self.b_f)
            
            # 输出门
            o_t = torch.sigmoid(x_t @ self.W_io.T + h_t @ self.W_ho.T + self.b_o)
            
            # 候选细胞状态
            g_t = torch.tanh(x_t @ self.W_ig.T + h_t @ self.W_hg.T + self.b_g)
            
            # 更新细胞状态
            c_t = f_t * c_t + i_t * g_t
            
            # 更新隐藏状态
            h_t = o_t * torch.tanh(c_t)
            
            output.append(h_t)
        
        return torch.stack(output, dim=1), (h_t, c_t)
```

### PyTorch GRU实现

```python
class GRU(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers=1):
        super(GRU, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # 更新门参数
        self.W_z = nn.Parameter(torch.Tensor(hidden_size, input_size + hidden_size))
        self.b_z = nn.Parameter(torch.Tensor(hidden_size))
        
        # 重置门参数
        self.W_r = nn.Parameter(torch.Tensor(hidden_size, input_size + hidden_size))
        self.b_r = nn.Parameter(torch.Tensor(hidden_size))
        
        # 候选隐藏状态参数
        self.W_h = nn.Parameter(torch.Tensor(hidden_size, input_size + hidden_size))
        self.b_h = nn.Parameter(torch.Tensor(hidden_size))
        
        self.init_weights()
    
    def init_weights(self):
        for p in self.parameters():
            if p.data.ndimension() >= 2:
                nn.init.xavier_uniform_(p.data)
            else:
                nn.init.zeros_(p.data)
    
    def forward(self, x, h_0=None):
        batch_size, seq_len, _ = x.size()
        
        if h_0 is None:
            h_t = torch.zeros(self.num_layers, batch_size, self.hidden_size)
        else:
            h_t = h_0
        
        output = []
        for t in range(seq_len):
            x_t = x[:, t, :]
            combined = torch.cat((x_t, h_t), dim=1)
            
            # 更新门
            z_t = torch.sigmoid(combined @ self.W_z.T + self.b_z)
            
            # 重置门
            r_t = torch.sigmoid(combined @ self.W_r.T + self.b_r)
            
            # 候选隐藏状态
            combined_reset = torch.cat((x_t, r_t * h_t), dim=1)
            h_tilde = torch.tanh(combined_reset @ self.W_h.T + self.b_h)
            
            # 更新隐藏状态
            h_t = (1 - z_t) * h_t + z_t * h_tilde
            
            output.append(h_t)
        
        return torch.stack(output, dim=1), h_t
```

## 相关论文

### 经典论文

1. **LSTM原始论文**
   - Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation, 9(8), 1735-1780.
   - 链接：(https://www.bioinf.jku.at/publications/older/2604.pdf)

2. **GRU论文**
   - Cho, K., et al. (2014). Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. EMNLP.
   - 链接：(https://arxiv.org/abs/1406.1078)

3. **LSTM改进论文**
   - Gers, F. A., Schmidhuber, J., & Cummins, F. (2000). Learning to Forget: Continual Prediction with LSTM. Neural Computation, 12(10), 2451-2471.

### 应用论文

4. **门控网络在NLP中的应用**
   - Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to Sequence Learning with Neural Networks. NIPS.
   - 链接：(https://arxiv.org/abs/1409.3215)

5. **门控注意力机制**
   - Dauphin, Y. N., et al. (2017). Language Modeling with Gated Convolutional Networks. ICML.
   - 链接：(https://arxiv.org/abs/1612.08083)

## 应用场景

- 自然语言处理（机器翻译、文本生成）
- 语音识别
- 时间序列预测
- 视频分析
- 推荐系统

## 优缺点

### 优点
- 解决梯度消失/爆炸问题
- 长期依赖建模能力强
- 信息流动可控

### 缺点
- 计算复杂度高
- 参数数量多
- 训练时间较长