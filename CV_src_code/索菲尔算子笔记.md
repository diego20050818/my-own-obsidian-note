# ç´¢è²å°”ç®—å­ (Sobel Operator) ç¬”è®°

## ğŸ“š åŸºæœ¬æ¦‚å¿µ

ç´¢è²å°”ç®—å­æ˜¯ä¸€ç§ç»å…¸çš„**è¾¹ç¼˜æ£€æµ‹**æ–¹æ³•ï¼Œç”¨äºåœ¨å›¾åƒä¸­æ‰¾åˆ°æ˜æš—å˜åŒ–æ˜æ˜¾çš„åŒºåŸŸï¼Œå°±åƒç»™å›¾åƒç”»è½®å»“ä¸€æ ·ï½(â—•â€¿â—•âœ¿)

### ğŸ§® æ•°å­¦åŸç†

ç´¢è²å°”ç®—å­ä½¿ç”¨ä¸€å¯¹**å·ç§¯æ ¸**æ¥æ£€æµ‹æ°´å¹³å’Œå‚ç›´æ–¹å‘çš„è¾¹ç¼˜ï¼š

**æ°´å¹³æ–¹å‘æ ¸ (æ£€æµ‹å‚ç›´è¾¹ç¼˜)**ï¼š
```
Gx = [[-1, 0, 1],
      [-2, 0, 2], 
      [-1, 0, 1]]
```

**å‚ç›´æ–¹å‘æ ¸ (æ£€æµ‹æ°´å¹³è¾¹ç¼˜)**ï¼š
```
Gy = [[-1, -2, -1],
      [ 0,  0,  0],
      [ 1,  2,  1]]
```

**è®¡ç®—è¿‡ç¨‹**ï¼š
1. ç”¨Gxå’ŒGyåˆ†åˆ«ä¸å›¾åƒå·ç§¯
2. è®¡ç®—æ¢¯åº¦å¹…å€¼ï¼š$G = \sqrt{G_x^2 + G_y^2}$
3. è®¡ç®—æ¢¯åº¦æ–¹å‘ï¼š$\theta = \arctan\left(\frac{G_y}{G_x}\right)$

## ğŸ’» ä»£ç å®ç°

### PyTorch å®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SobelOperator(nn.Module):
    """ç´¢è²å°”ç®—å­å®ç°"""
    
    def __init__(self, in_channels=1):
        super().__init__()
        
        # å®šä¹‰ç´¢è²å°”å·ç§¯æ ¸
        sobel_x = torch.tensor([[-1, 0, 1],
                                [-2, 0, 2],
                                [-1, 0, 1]], dtype=torch.float32)
        
        sobel_y = torch.tensor([[-1, -2, -1],
                                [ 0,  0,  0],
                                [ 1,  2,  1]], dtype=torch.float32)
        
        # æ‰©å±•ç»´åº¦ä»¥é€‚åº”å·ç§¯å±‚
        sobel_x = sobel_x.view(1, 1, 3, 3).repeat(in_channels, 1, 1, 1)
        sobel_y = sobel_y.view(1, 1, 3, 3).repeat(in_channels, 1, 1, 1)
        
        # æ³¨å†Œä¸ºbufferï¼ˆä¸å‚ä¸æ¢¯åº¦æ›´æ–°ï¼‰
        self.register_buffer('sobel_x', sobel_x)
        self.register_buffer('sobel_y', sobel_y)
        
        self.in_channels = in_channels
    
    def forward(self, x):
        """
        å‚æ•°:
            x: è¾“å…¥å›¾åƒ [B, C, H, W]
        
        è¿”å›:
            gradient_magnitude: æ¢¯åº¦å¹…å€¼ [B, C, H, W]
            gradient_direction: æ¢¯åº¦æ–¹å‘ [B, C, H, W]
        """
        batch_size, channels, height, width = x.shape
        
        # åˆ†åˆ«è®¡ç®—æ°´å¹³å’Œå‚ç›´æ¢¯åº¦
        gx = F.conv2d(x, self.sobel_x, padding=1, groups=channels)
        gy = F.conv2d(x, self.sobel_y, padding=1, groups=channels)
        
        # è®¡ç®—æ¢¯åº¦å¹…å€¼å’Œæ–¹å‘
        gradient_magnitude = torch.sqrt(gx**2 + gy**2)
        gradient_direction = torch.atan2(gy, gx)  # å¼§åº¦åˆ¶
        
        return gradient_magnitude, gradient_direction
```

### OpenCV å®ç°

```python
import cv2
import numpy as np

def sobel_opencv(image_path):
    """ä½¿ç”¨OpenCVå®ç°ç´¢è²å°”ç®—å­"""
    # è¯»å–å›¾åƒ
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    
    # ç´¢è²å°”ç®—å­
    sobelx = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=3)
    sobely = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=3)
    
    # è®¡ç®—æ¢¯åº¦å¹…å€¼
    gradient_magnitude = np.sqrt(sobelx**2 + sobely**2)
    
    return gradient_magnitude, sobelx, sobely
```

## ğŸ¯ åœ¨é‡‘å­—å¡”æ¨¡å‹ä¸­çš„åº”ç”¨

### é›†æˆåˆ°å±€éƒ¨ç»†èŠ‚åˆ†æ”¯

```python
class LocalDetailBranch(nn.Module):
    """å±€éƒ¨ç»†èŠ‚åˆ†æ”¯ - ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶"""
    
    def __init__(self, dim):
        super().__init__()
        self.spatial_attention = SpatialAttention(dim)
        self.sobel_operator = SobelOperator()  # æ–°å¢ç´¢è²å°”ç®—å­
        
    def forward(self, features_list):
        attended_features = []
        for feat in features_list:
            # è®¡ç®—è¾¹ç¼˜ç‰¹å¾
            edge_magnitude, _ = self.sobel_operator(feat)
            
            # ç»“åˆç©ºé—´æ³¨æ„åŠ›
            attended = self.spatial_attention(feat)
            
            # å¢å¼ºè¾¹ç¼˜åŒºåŸŸçš„ç‰¹å¾å“åº”
            enhanced = attended * (1 + edge_magnitude)
            
            pooled = F.adaptive_avg_pool2d(enhanced, 1).view(enhanced.size(0), -1)
            attended_features.append(pooled)
        
        fused = torch.stack(attended_features, dim=1).mean(dim=1)
        return fused
```

### åº”ç”¨æ•ˆæœ

1. **è¾¹ç¼˜å¢å¼º**: è®©æ¨¡å‹æ›´å…³æ³¨å›¾åƒè¾¹ç•Œä¿¡æ¯
2. **ä¼ªé€ æ£€æµ‹**: æ·±åº¦ä¼ªé€ å›¾åƒå¾€å¾€åœ¨è¾¹ç¼˜å¤„æœ‰ä¸è‡ªç„¶çš„è¿‡æ¸¡
3. **å¤šå°ºåº¦åˆ†æ**: åœ¨ä¸åŒé‡‘å­—å¡”å±‚çº§æ£€æµ‹ä¸åŒç²—ç»†çš„è¾¹ç¼˜

## ğŸ“– ç›¸å…³è®ºæ–‡

### ç»å…¸è®ºæ–‡
- **[Sobel, I. (1970). An Isotropic 3Ã—3 Gradient Operator](https://www.researchgate.net/publication/239398674_An_Isotropic_3x3_Gradient_Operator)** - ç´¢è²å°”ç®—å­çš„åŸå§‹è®ºæ–‡
- **[Canny, J. (1986). A Computational Approach to Edge Detection](https://ieeexplore.ieee.org/document/4767851)** - ç»å…¸çš„è¾¹ç¼˜æ£€æµ‹ç®—æ³•


### ç°ä»£åº”ç”¨
- **[Edge-aware graph matching network for part-based semantic segmentation](https://link.springer.com/article/10.1007/s11263-022-01671-z)** - è¾¹ç¼˜æ„ŸçŸ¥çš„å›¾åŒ¹é…ç½‘ç»œç”¨äºéƒ¨åˆ†è¯­ä¹‰åˆ†å‰²
- 
```embed
title: "Edge-Aware Graph Matching Network for Part-Based Semantic Segmentation - International Journal of Computer Vision"
image: "https://static-content.springer.com/image/art%3A10.1007%2Fs11263-022-01671-z/MediaObjects/11263_2022_1671_Fig1_HTML.png"
description: "Semantic segmentation of parts of objects is a marginally explored and challenging task in which multiple instances of objects and multiple parts within those objects must be recognized in an image. We introduce a novel approach (GMENet) for this task combining object-level context conditioning, part-level spatial relationships, and shape contour information. The first target is achieved by introducing a class-conditioning module that enforces class-level semantics when learning the part-level ones. Thus, intermediate-level features carry object-level prior to the decoding stage. To tackle part-level ambiguity and spatial relationships among parts we exploit an adjacency graph-based module that aims at matching the spatial relationships between parts in the ground truth and predicted maps. Last, we introduce an additional module to further leverage edges localization. Besides testing our framework on the already used Pascal-Part-58 and Pascal-Person-Part benchmarks, we further introduce two novel benchmarks for large-scale part parsing, i.e., a more challenging version of..."
url: "https://link.springer.com/article/10.1007/s11263-022-01671-z"
favicon: ""
aspectRatio: "29.780033840947546"
```
- **[Edge-enhanced feature distillation network for efficient super-resolution](https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Wang_Edge-Enhanced_Feature_Distillation_Network_for_Efficient_Super-Resolution_CVPRW_2022_paper.html)** - è¾¹ç¼˜å¢å¼ºçš„ç‰¹å¾è’¸é¦ç½‘ç»œç”¨äºé«˜æ•ˆè¶…åˆ†è¾¨ç‡
```embed
title: "CVPR 2022 Open Access Repository"
image: "https://openaccess.thecvf.com/img/cvpr2022.png"
description: "These CVPR 2022 workshop papers are the Open Access versions, provided by the Except for the watermark, they are identical to the accepted versions; the final published version of the proceedings is available on IEEE Xplore."
url: "https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Wang_Edge-Enhanced_Feature_Distillation_Network_for_Efficient_Super-Resolution_CVPRW_2022_paper.html"
favicon: ""
aspectRatio: "22.783143107989464"
```

- **[Edge-guided multi-scale adaptive feature fusion network for liver tumor segmentation](https://www.nature.com/articles/s41598-024-79379-y)** - è¾¹ç¼˜å¼•å¯¼çš„å¤šå°ºåº¦è‡ªé€‚åº”ç‰¹å¾èåˆç½‘ç»œç”¨äºè‚è‚¿ç˜¤åˆ†å‰²
```embed
title: "Edge-guided multi-scale adaptive feature fusion network for liver tumor segmentation - Scientific Reports"
image: "https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs41598-024-79379-y/MediaObjects/41598_2024_79379_Fig1_HTML.png"
description: "Automated segmentation of liver tumors on CT scans is essential for aiding diagnosis and assessing treatment. Computer-aided diagnosis can reduce the costs and errors associated with manual processes and ensure the provision of accurate and reliable clinical assessments. However, liver tumors in CT images vary significantly in size and have fuzzy boundaries, making it difficult for existing methods to achieve accurate segmentation. Therefore, this paper proposes MAEG-Net, a multi-scale adaptive feature fusion liver tumor segmentation network based on edge guidance. Specifically, we design a multi-scale adaptive feature fusion module that effectively incorporates multi-scale information to better guide the segmentation of tumors of different sizes. Additionally, to address the problem of blurred tumor boundaries in images, we introduce an edge-aware guidance module to improve the model's feature learning ability under these conditions. Evaluation results on the liver tumor dataset (LiTS2017) show that our method achieves a Dice coefficient of 71.84% and..."
url: "https://www.nature.com/articles/s41598-024-79379-y"
favicon: ""
```



## ğŸ”— GitHub ä»“åº“

### å®˜æ–¹å®ç°
- **[OpenCV Sobel Operator](https://github.com/opencv/opencv/blob/master/modules/imgproc/src/deriv.cpp)** - OpenCVä¸­çš„ç´¢è²å°”ç®—å­å®ç°
```embed
title: "opencv/modules/imgproc/src/deriv.cpp at master Â· opencv/opencv"
image: "https://opengraph.githubassets.com/59be68513ad270a65afe0ab8c97a769329c37971e4e10d4173e9ae1a99abf167/opencv/opencv"
description: "Open Source Computer Vision Library. Contribute to opencv/opencv development by creating an account on GitHub."
url: "https://github.com/opencv/opencv/blob/master/modules/imgproc/src/deriv.cpp"
favicon: ""
aspectRatio: "50"
```
- **[PyTorch Vision](https://github.com/pytorch/vision/blob/main/torchvision/transforms/functional.py)** - PyTorchè§†è§‰åº“ä¸­çš„ç›¸å…³å®ç°
```embed
title: "vision/torchvision/transforms/functional.py at main Â· pytorch/vision"
image: "https://opengraph.githubassets.com/a6a7d512acc3b0396deb7f9e8c989a69b1418e3594fb28b5cd4f7ae63f172729/pytorch/vision"
description: "Datasets, Transforms and Models specific to Computer Vision - pytorch/vision"
url: "https://github.com/pytorch/vision/blob/main/torchvision/transforms/functional.py"
favicon: ""
aspectRatio: "50"
```

### ç¬¬ä¸‰æ–¹å®ç°
- **[Sobel-Edge-Detector](https://github.com/justinliang1020/Sobel-Edge-Detector)** - çº¯Pythonå®ç°çš„ç´¢è²å°”è¾¹ç¼˜æ£€æµ‹å™¨
- **[Deep-Edge-Detection](https://github.com/s9xie/hed)** - åŸºäºæ·±åº¦å­¦ä¹ çš„è¾¹ç¼˜æ£€æµ‹
```embed
title: "GitHub - s9xie/hed: code for Holistically-Nested Edge Detection"
image: "https://opengraph.githubassets.com/d9cd054ef3f94d750241cd0f3e0825729419536c06b559b428d4a5091c80cb71/s9xie/hed"
description: "code for Holistically-Nested Edge Detection. Contribute to s9xie/hed development by creating an account on GitHub."
url: "https://github.com/s9xie/hed"
favicon: ""
aspectRatio: "50"
```

## ğŸŒŸ ä¼˜åŠ¿ä¸å±€é™

### ä¼˜åŠ¿
- âœ… **è®¡ç®—ç®€å•**: ä»…éœ€3Ã—3å·ç§¯æ ¸
- âœ… **æ–¹å‘æ•æ„Ÿ**: èƒ½æ£€æµ‹æ°´å¹³å’Œå‚ç›´è¾¹ç¼˜
- âœ… **å®æ—¶æ€§å¥½**: é€‚åˆå®æ—¶åº”ç”¨
- âœ… **å™ªå£°é²æ£’**: å¯¹å™ªå£°æœ‰ä¸€å®šçš„æŠ‘åˆ¶èƒ½åŠ›

### å±€é™
- âŒ **æ–¹å‘æœ‰é™**: åªèƒ½æ£€æµ‹æ°´å¹³å’Œå‚ç›´æ–¹å‘
- âŒ **è¾¹ç¼˜ç²—åŒ–**: å¯èƒ½äº§ç”Ÿè¾ƒç²—çš„è¾¹ç¼˜
- âŒ **å™ªå£°æ•æ„Ÿ**: å¯¹å¼ºå™ªå£°ä»ç„¶æ•æ„Ÿ

## ğŸ’¡ å°è´´å£«

å¼€æ‹“è€…å¯ä»¥è¿™æ ·ç†è§£ç´¢è²å°”ç®—å­ï¼š
- å°±åƒç”¨**æ”¾å¤§é•œ**çœ‹å›¾åƒçš„è½®å»“çº¿
- æ°´å¹³æ ¸æ‰¾**å‚ç›´è¾¹ç¼˜**ï¼ˆæ¯”å¦‚å»ºç­‘ç‰©çš„ç«–çº¿ï¼‰
- å‚ç›´æ ¸æ‰¾**æ°´å¹³è¾¹ç¼˜**ï¼ˆæ¯”å¦‚åœ°å¹³çº¿ï¼‰
- åœ¨é‡‘å­—å¡”çš„ä¸åŒå±‚çº§ï¼Œèƒ½æ£€æµ‹åˆ°**ä¸åŒç²—ç»†**çš„è¾¹ç¼˜ç‰¹å¾ï½


---
*åˆ›å»ºæ—¶é—´: 2024å¹´*  
*æœ€åæ›´æ–°: 2024å¹´*