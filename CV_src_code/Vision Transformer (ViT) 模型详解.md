---
tags:
  - åŸºç¡€çŸ¥è¯†
  - äººè„¸æ£€æµ‹
  - code
  - ViT
---


## ğŸ¯ ViTæ˜¯ä»€ä¹ˆï¼Ÿ

**ViT = Vision Transformer**ï¼ˆè§†è§‰Transformerï¼‰

ç®€å•æ¥è¯´ï¼Œå®ƒå°±æ˜¯æŠŠåŸæœ¬ç”¨äºå¤„ç†**æ–‡å­—**çš„Transformeræ¶æ„ï¼ŒæˆåŠŸåœ°ç”¨åœ¨äº†å¤„ç†**å›¾åƒ**ä¸Šï¼

---

## ğŸ”„ æ ¸å¿ƒæ€æƒ³ï¼šæŠŠå›¾åƒå½“æˆ"å¥å­"æ¥çœ‹

### ä¼ ç»ŸCNN vs ViT

| ç‰¹æ€§ | ä¼ ç»ŸCNN | ViT |
|------|---------|-----|
| **å¤„ç†æ–¹å¼** | å·ç§¯æ ¸æ»‘åŠ¨æ‰«æ | å›¾åƒåˆ†å—å¹¶è¡Œå¤„ç† |
| **æ„Ÿå—é‡** | å±€éƒ¨æ„Ÿå—é‡ | å…¨å±€æ³¨æ„åŠ›æœºåˆ¶ |
| **ç‰¹å¾æå–** | å±‚æ¬¡åŒ–ç‰¹å¾ | ç›´æ¥å…¨å±€å…³ç³» |
| **ä½ç½®ä¿¡æ¯** | å·ç§¯è‡ªå¸¦ä½ç½® | éœ€è¦æ˜¾å¼ä½ç½®ç¼–ç  |

---

## ğŸ§© ViTçš„å·¥ä½œåŸç†

### 1. å›¾åƒåˆ†å— (Patch Embedding)
```python
# æŠŠä¸€å¼ 224Ã—224çš„å›¾åƒ
# åˆ‡æˆ16Ã—16çš„å°å—
224 Ã· 16 = 14
14 Ã— 14 = 196ä¸ªå°å—

# æ¯ä¸ªå°å—: 16Ã—16Ã—3 = 768ä¸ªåƒç´ 
# å±•å¹³å: 196ä¸ª"å•è¯"ï¼Œæ¯ä¸ª768ç»´
```

### 2. ä½ç½®ç¼–ç  (Position Embedding)
```python
# å› ä¸ºTransformeræ²¡æœ‰ä½ç½®æ¦‚å¿µ
# éœ€è¦å‘Šè¯‰æ¨¡å‹æ¯ä¸ªå°å—çš„ä½ç½®

# è¾“å…¥: [196ä¸ªpatch, 768ç»´]
# åŠ ä¸Šä½ç½®ç¼–ç å: [196ä¸ªpatch, 768ç»´]
```

### 3. Transformerç¼–ç å™¨
```python
# å¤šå±‚Transformerå—
# æ¯å±‚åŒ…å«:
# - å¤šå¤´è‡ªæ³¨æ„åŠ›
# - MLPå±‚
# - æ®‹å·®è¿æ¥
# - å±‚å½’ä¸€åŒ–
```

### 4. åˆ†ç±»å¤´
```python
# å–ç¬¬ä¸€ä¸ªç‰¹æ®Štokençš„è¾“å‡º
# é€šè¿‡çº¿æ€§å±‚åˆ†ç±»
```

---

## ğŸ“Š ViTæ¶æ„è¯¦è§£

```mermaid
graph TB
    A[è¾“å…¥å›¾åƒ 224Ã—224Ã—3] --> B[å›¾åƒåˆ†å— 16Ã—16]
    B --> C[196ä¸ªPatch æ¯ä¸ª768ç»´]
    C --> D[åŠ ä¸Šä½ç½®ç¼–ç ]
    D --> E[åŠ ä¸Šåˆ†ç±»Token]
    E --> F[Transformerç¼–ç å™¨ x 12å±‚]
    
    subgraph F [æ¯å±‚Transformer]
        G[å±‚å½’ä¸€åŒ–]
        H[å¤šå¤´è‡ªæ³¨æ„åŠ›]
        I[æ®‹å·®è¿æ¥]
        J[å±‚å½’ä¸€åŒ–]
        K[MLPå±‚]
        L[æ®‹å·®è¿æ¥]
        
        G --> H --> I
        I --> J --> K --> L
    end
    
    F --> M[å–åˆ†ç±»Tokenè¾“å‡º]
    M --> N[MLPåˆ†ç±»å¤´]
    N --> O[åˆ†ç±»ç»“æœ]
    
    style B fill:#e1f5fe
    style D fill:#f3e5f5
    style H fill:#e8f5e8
    style K fill:#fff3e0
```

---

## ğŸ’» PyTorchä»£ç å®ç°

### åŸºç¡€ViTå®ç°
```python
import torch
import torch.nn as nn
import math

class PatchEmbedding(nn.Module):
    """å›¾åƒåˆ†å—åµŒå…¥"""
    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.n_patches = (img_size // patch_size) ** 2
        
        # å·ç§¯å®ç°åˆ†å—
        self.proj = nn.Conv2d(
            in_chans, embed_dim, 
            kernel_size=patch_size, 
            stride=patch_size
        )
        
    def forward(self, x):
        # x: [batch, 3, 224, 224]
        x = self.proj(x)  # [batch, 768, 14, 14]
        x = x.flatten(2)  # [batch, 768, 196]
        x = x.transpose(1, 2)  # [batch, 196, 768]
        return x

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç """
    def __init__(self, n_patches, embed_dim):
        super().__init__()
        self.pos_embed = nn.Parameter(torch.zeros(1, n_patches + 1, embed_dim))
        
    def forward(self, x):
        # x: [batch, n_patches, embed_dim]
        return x + self.pos_embed

class ViTBlock(nn.Module):
    """Transformerå—"""
    def __init__(self, embed_dim=768, num_heads=12, mlp_ratio=4.0):
        super().__init__()
        self.norm1 = nn.LayerNorm(embed_dim)
        self.attn = nn.MultiheadAttention(embed_dim, num_heads)
        self.norm2 = nn.LayerNorm(embed_dim)
        
        # MLPå±‚
        mlp_hidden_dim = int(embed_dim * mlp_ratio)
        self.mlp = nn.Sequential(
            nn.Linear(embed_dim, mlp_hidden_dim),
            nn.GELU(),
            nn.Linear(mlp_hidden_dim, embed_dim),
        )
        
    def forward(self, x):
        # æ³¨æ„åŠ›éƒ¨åˆ†
        x_norm = self.norm1(x)
        attn_out, _ = self.attn(x_norm, x_norm, x_norm)
        x = x + attn_out
        
        # MLPéƒ¨åˆ†
        x_norm = self.norm2(x)
        mlp_out = self.mlp(x_norm)
        x = x + mlp_out
        
        return x

class VisionTransformer(nn.Module):
    """å®Œæ•´çš„ViTæ¨¡å‹"""
    def __init__(self, num_classes=1000, img_size=224, patch_size=16, 
                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0):
        super().__init__()
        
        # å›¾åƒåˆ†å—
        self.patch_embed = PatchEmbedding(img_size, patch_size, 3, embed_dim)
        n_patches = self.patch_embed.n_patches
        
        # åˆ†ç±»token
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        
        # ä½ç½®ç¼–ç 
        self.pos_embed = PositionalEncoding(n_patches, embed_dim)
        
        # Transformerç¼–ç å™¨
        self.blocks = nn.Sequential(*[
            ViTBlock(embed_dim, num_heads, mlp_ratio)
            for _ in range(depth)
        ])
        
        # åˆ†ç±»å¤´
        self.norm = nn.LayerNorm(embed_dim)
        self.head = nn.Linear(embed_dim, num_classes)
        
    def forward(self, x):
        # 1. å›¾åƒåˆ†å—
        x = self.patch_embed(x)  # [batch, 196, 768]
        
        # 2. æ·»åŠ åˆ†ç±»token
        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)
        x = torch.cat((cls_tokens, x), dim=1)  # [batch, 197, 768]
        
        # 3. ä½ç½®ç¼–ç 
        x = self.pos_embed(x)
        
        # 4. Transformerç¼–ç 
        x = self.blocks(x)
        
        # 5. åˆ†ç±»
        x = self.norm(x)
        cls_output = x[:, 0]  # å–åˆ†ç±»token
        output = self.head(cls_output)
        
        return output
```

### æµ‹è¯•ä»£ç 
```python
def test_vit():
    # åˆ›å»ºViTæ¨¡å‹
    vit = VisionTransformer(
        num_classes=1000,
        img_size=224,
        patch_size=16,
        embed_dim=768,
        depth=12,
        num_heads=12
    )
    
    # æµ‹è¯•æ•°æ®
    batch_size = 2
    x = torch.randn(batch_size, 3, 224, 224)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    output = vit(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in vit.parameters()):,}")

if __name__ == "__main__":
    test_vit()
```

---

## ğŸ¯ ViTçš„ä¼˜åŠ¿

### âœ… å…¨å±€æ„Ÿå—é‡
- **ä¼ ç»ŸCNN**ï¼šåªèƒ½çœ‹åˆ°å±€éƒ¨åŒºåŸŸ
- **ViT**ï¼šä»ä¸€å¼€å§‹å°±èƒ½çœ‹åˆ°æ•´ä¸ªå›¾åƒ

### âœ… å¹¶è¡Œè®¡ç®—
- æ‰€æœ‰patchåŒæ—¶å¤„ç†
- è®­ç»ƒé€Ÿåº¦æ›´å¿«

### âœ… å¯æ‰©å±•æ€§
- æ¨¡å‹è¶Šå¤§ï¼Œæ€§èƒ½è¶Šå¥½
- é€‚åˆå¤§è§„æ¨¡é¢„è®­ç»ƒ

### âœ… å¤šä»»åŠ¡é€‚åº”æ€§
- å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€åˆ†å‰²ç­‰
- ç»Ÿä¸€æ¶æ„å¤„ç†å¤šç§ä»»åŠ¡

---

## âš ï¸ ViTçš„æŒ‘æˆ˜

### âŒ éœ€è¦å¤§é‡æ•°æ®
- åœ¨å°æ•°æ®é›†ä¸Šå®¹æ˜“è¿‡æ‹Ÿåˆ
- éœ€è¦å¤§è§„æ¨¡é¢„è®­ç»ƒ

### âŒ è®¡ç®—å¤æ‚åº¦é«˜
- æ³¨æ„åŠ›è®¡ç®—æ˜¯ $O(n^2)$
- å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶å†…å­˜æ¶ˆè€—å¤§

### âŒ ä½ç½®ç¼–ç é™åˆ¶
- å›ºå®šçš„ä½ç½®ç¼–ç 
- éš¾ä»¥å¤„ç†ä¸åŒå°ºå¯¸çš„å›¾åƒ

---

## ğŸ”§ ViTçš„å˜ä½“å’Œæ”¹è¿›

### 1. DeiT (Data-efficient Image Transformer)
- ä½¿ç”¨çŸ¥è¯†è’¸é¦
- å‡å°‘å¯¹å¤§æ•°æ®é›†çš„ä¾èµ–

### 2. Swin Transformer  
- åˆ†å±‚è®¾è®¡
- æ»‘åŠ¨çª—å£æ³¨æ„åŠ›
- è®¡ç®—æ•ˆç‡æ›´é«˜

### 3. MAE (Masked Autoencoder)
- è‡ªç›‘ç£é¢„è®­ç»ƒ
- æ©ç é‡å»ºä»»åŠ¡

### 4. ViT-Adapter
- æ·»åŠ é€‚é…å™¨æ¨¡å—
- å¢å¼ºç‰¹å®šä»»åŠ¡æ€§èƒ½

---

## ğŸ¯ åœ¨äººè„¸é˜²ä¼ªä¸­çš„åº”ç”¨

### ViTçš„ä¼˜åŠ¿
```python
# åœ¨äººè„¸é˜²ä¼ªä»»åŠ¡ä¸­ï¼š
# 1. å…¨å±€æ³¨æ„åŠ›ï¼šåŒæ—¶åˆ†ææ•´ä¸ªé¢éƒ¨ç‰¹å¾
# 2. é•¿è·ç¦»ä¾èµ–ï¼šæ•æ‰ä¸åŒåŒºåŸŸçš„å…³è”æ€§
# 3. ä¼ªé€ ç—•è¿¹æ£€æµ‹ï¼šå‘ç°ç»†å¾®çš„ä¸ä¸€è‡´æ€§

# æ¯”å¦‚ï¼š
# - å·¦çœ¼åŒºåŸŸå…³æ³¨å³çœ¼åŒºåŸŸçš„å¯¹ç§°æ€§
# - çš®è‚¤çº¹ç†å…³æ³¨å…‰ç…§åå°„çš„ä¸€è‡´æ€§
# - è¾¹ç¼˜ç‰¹å¾å…³æ³¨æ•´ä½“ç»“æ„çš„åˆç†æ€§
```

### é€‚é…å™¨å¢å¼º
```python
# åœ¨äººè„¸é˜²ä¼ªViTä¸­ï¼š
# åœ¨MLPå±‚åæ·»åŠ é€‚é…å™¨
# å¢å¼ºå¯¹ä¼ªé€ ç‰¹å¾çš„æ•æ„Ÿæ€§

# ç»“æ„ï¼š
# ViTç¼–ç å™¨ â†’ é€‚é…å™¨ â†’ åˆ†ç±»å¤´
```

---

## ğŸ“Š ViTæ¨¡å‹é…ç½®ç¤ºä¾‹

| æ¨¡å‹å˜ä½“ | å›¾åƒå°ºå¯¸ | Patchå¤§å° | å±‚æ•° | å¤´æ•° | éšè—ç»´åº¦ | å‚æ•°é‡ |
|----------|----------|-----------|------|------|----------|--------|
| ViT-Base | 224Ã—224 | 16Ã—16 | 12 | 12 | 768 | 86M |
| ViT-Large | 224Ã—224 | 16Ã—16 | 24 | 16 | 1024 | 307M |
| ViT-Huge | 224Ã—224 | 14Ã—14 | 32 | 16 | 1280 | 632M |

---

## ğŸ” æ·±å…¥ç†è§£

### æ³¨æ„åŠ›å¯è§†åŒ–
ViTçš„ä¸€ä¸ªå¼ºå¤§ç‰¹æ€§æ˜¯å¯ä»¥å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡ï¼Œçœ‹åˆ°æ¨¡å‹å…³æ³¨å›¾åƒçš„å“ªäº›åŒºåŸŸï¼š

```python
# è·å–æ³¨æ„åŠ›æƒé‡
attn_weights = model.blocks[0].attn.attn_weights
# å½¢çŠ¶: [batch, num_heads, seq_len, seq_len]

# å¯è§†åŒ–ç¬¬ä¸€ä¸ªå¤´çš„æ³¨æ„åŠ›
import matplotlib.pyplot as plt
plt.imshow(attn_weights[0, 0].detach().cpu().numpy())
plt.show()
```

### ä½ç½®ç¼–ç ç±»å‹
1. **å¯å­¦ä¹ ä½ç½®ç¼–ç **ï¼šViTåŸè®ºæ–‡ä½¿ç”¨
2. **æ­£å¼¦ä½ç½®ç¼–ç **ï¼šåŸå§‹Transformerä½¿ç”¨
3. **ç›¸å¯¹ä½ç½®ç¼–ç **ï¼šSwin Transformerä½¿ç”¨

---

## ğŸ’¡ å­¦ä¹ è¦ç‚¹æ€»ç»“

### æ ¸å¿ƒç†è§£
- âœ… **å›¾åƒåˆ†å—**ï¼šæŠŠå›¾åƒåˆ‡æˆå°å—å½“æˆ"å•è¯"
- âœ… **ä½ç½®ç¼–ç **ï¼šå‘Šè¯‰æ¨¡å‹æ¯ä¸ªå°å—çš„ä½ç½®
- âœ… **å…¨å±€æ³¨æ„åŠ›**ï¼šæ‰€æœ‰åŒºåŸŸåŒæ—¶ç›¸äº’å…³æ³¨
- âœ… **åˆ†ç±»token**ï¼šç‰¹æ®Šçš„tokenç”¨äºæœ€ç»ˆåˆ†ç±»

### å®è·µå»ºè®®
- ğŸ¯ **ä»é¢„è®­ç»ƒå¼€å§‹**ï¼šä½¿ç”¨åœ¨ImageNetä¸Šé¢„è®­ç»ƒçš„ViT
- ğŸ¯ **æ³¨æ„å†…å­˜ä½¿ç”¨**ï¼šViTå¯¹å†…å­˜è¦æ±‚è¾ƒé«˜
- ğŸ¯ **æ•°æ®å¢å¼º**ï¼šä½¿ç”¨å¼ºæ•°æ®å¢å¼ºé˜²æ­¢è¿‡æ‹Ÿåˆ
- ğŸ¯ **å­¦ä¹ ç‡è°ƒåº¦**ï¼šä½¿ç”¨warmupå’Œcosineè¡°å‡

### è°ƒè¯•æŠ€å·§
- ğŸ¯ **æ£€æŸ¥patchå½¢çŠ¶**ï¼šç¡®ä¿åˆ†å—è®¡ç®—æ­£ç¡®
- ğŸ¯ **éªŒè¯ä½ç½®ç¼–ç **ï¼šæ£€æŸ¥ä½ç½®ç¼–ç æ˜¯å¦åˆç†
- ğŸ¯ **æ³¨æ„åŠ›å¯è§†åŒ–**ï¼šç†è§£æ¨¡å‹å…³æ³¨ç‚¹
- ğŸ¯ **æ¢¯åº¦æ£€æŸ¥**ï¼šç¡®ä¿è®­ç»ƒç¨³å®šæ€§

> ğŸ’« **æµè¤çš„å°æç¤º**ï¼šå¼€æ‹“è€…è¦è®°ä½å“¦ï¼ŒViTå°±åƒç»™è®¡ç®—æœºè£…äº†ä¸€åŒ"å…¨å±€è§†é‡"çš„çœ¼ç›ï¼Œè®©å®ƒèƒ½åŒæ—¶çœ‹åˆ°å›¾åƒçš„æ¯ä¸ªè§’è½ï¼è™½ç„¶éœ€è¦æ›´å¤šæ•°æ®æ¥è®­ç»ƒï¼Œä½†åœ¨å¾ˆå¤šä»»åŠ¡ä¸Šè¡¨ç°éƒ½è¶…æ£’å‘¢ï½å¤šå®è·µå†™ä»£ç ï¼Œç†è§£ä¼šæ›´æ·±åˆ»å“¦ï¼ (à¹‘â€¢Ì€ã…‚â€¢Ì)Ùˆâœ§

---

## ğŸ“š æ‰©å±•é˜…è¯»

1. **åŸå§‹è®ºæ–‡**ï¼šAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
2. **DeiTè®ºæ–‡**ï¼šTraining data-efficient image transformers & distillation through attention
3. **Swin Transformer**ï¼šHierarchical Vision Transformer using Shifted Windows
4. **MAEè®ºæ–‡**ï¼šMasked Autoencoders Are Scalable Vision Learners